{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Principales Metricas de modelos supervisados\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Para el modelo en general ###\n",
    "### Exactitud (Accuracy)\n",
    "\n",
    "La **exactitud** mide el porcentaje de predicciones correctas del modelo sobre el total de predicciones realizadas. Se calcula utilizando la fórmula:\n",
    "\n",
    "\\[\n",
    "\\text{Exactitud} = \\frac{TP + TN}{TP + TN + FP + FN}\n",
    "\\]\n",
    "\n",
    "donde:\n",
    "- **TP** (True Positives): Verdaderos positivos, casos en los que el modelo predijo correctamente la clase positiva.\n",
    "- **TN** (True Negatives): Verdaderos negativos, casos en los que el modelo predijo correctamente la clase negativa.\n",
    "- **FP** (False Positives): Falsos positivos, casos en los que el modelo predijo la clase positiva incorrectamente.\n",
    "- **FN** (False Negatives): Falsos negativos, casos en los que el modelo predijo la clase negativa incorrectamente.\n",
    "\n",
    "#### Ejemplo de Cálculo\n",
    "Si tenemos los siguientes valores de la matriz de confusión:\n",
    "- **TP** = 52\n",
    "- **TN** = 100\n",
    "- **FP** = 5\n",
    "- **FN** = 14\n",
    "\n",
    "Entonces, la exactitud se calcula así:\n",
    "\n",
    "\\[\n",
    "\\text{Exactitud} = \\frac{52 + 100}{52 + 100 + 5 + 14} = \\frac{152}{171} \\approx 88.89\\%\n",
    "\\]\n",
    "\n",
    "Esto se desglosa de la siguiente manera:\n",
    "\n",
    "\\[\n",
    "\\text{Exactitud} = \\frac{152}{171} \\approx 0.8906 \\approx 89.06\\%\n",
    "\\]\n",
    "\n",
    "La exactitud nos indica que, en este ejemplo, el modelo tiene aproximadamente un 89.06% de predicciones correctas.\n",
    "\n",
    "Desde aqui nace la **Matriz de confusión** y otras metricas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Análisis de la Matriz de Confusión\n",
    "\n",
    "### 1. Matriz de Confusión\n",
    "La matriz de confusión del modelo muestra la clasificación de dos clases: *malignant* y *benign*. Se observa la siguiente distribución:\n",
    "\n",
    "|             | **True Malignant** | **True Benign** |\n",
    "|-------------|--------------------|-----------------|\n",
    "| **Predicted Malignant** | 52                 | 5               |\n",
    "| **Predicted Benign**    | 14                 | 100             |\n",
    "\n",
    "### 2. Métricas de Evaluación\n",
    "A partir de la matriz de confusión, se pueden calcular las siguientes métricas de evaluación del modelo:\n",
    "\n",
    "#### a. Exactitud (Accuracy)\n",
    "La exactitud indica el porcentaje de predicciones correctas.\n",
    "\n",
    "\\[\n",
    "\\text{Exactitud} = \\frac{TP + TN}{TP + TN + FP + FN}\n",
    "\\]\n",
    "\n",
    "\\[\n",
    "\\text{Exactitud} = \\frac{52 + 100}{52 + 100 + 5 + 14} = \\frac{152}{171} \\approx 88.89\\%\n",
    "\\]\n",
    "\n",
    "#### b. Precisión (Precision)\n",
    "La precisión evalúa qué tan exactos son los positivos predichos.\n",
    "\n",
    "- **Para *Malignant***:\n",
    "\n",
    "\\[\n",
    "\\text{Precisión}_{\\text{Malignant}} = \\frac{TP}{TP + FP} = \\frac{52}{52 + 5} = \\frac{52}{57} \\approx 91.23\\%\n",
    "\\]\n",
    "\n",
    "- **Para *Benign***:\n",
    "\n",
    "\\[\n",
    "\\text{Precisión}_{\\text{Benign}} = \\frac{TN}{TN + FN} = \\frac{100}{100 + 14} = \\frac{100}{114} \\approx 87.72\\%\n",
    "\\]\n",
    "\n",
    "#### c. Sensibilidad (Recall)\n",
    "La sensibilidad mide la capacidad del modelo para identificar correctamente los verdaderos positivos.\n",
    "\n",
    "- **Para *Malignant***:\n",
    "\n",
    "\\[\n",
    "\\text{Sensibilidad}_{\\text{Malignant}} = \\frac{TP}{TP + FN} = \\frac{52}{52 + 14} = \\frac{52}{66} \\approx 78.79\\%\n",
    "\\]\n",
    "\n",
    "- **Para *Benign***:\n",
    "\n",
    "\\[\n",
    "\\text{Sensibilidad}_{\\text{Benign}} = \\frac{TN}{TN + FP} = \\frac{100}{100 + 5} = \\frac{100}{105} \\approx 95.24\\%\n",
    "\\]\n",
    "\n",
    "#### d. F1-Score\n",
    "El F1-Score es la media armónica entre precisión y sensibilidad.\n",
    "\n",
    "- **Para *Malignant***:\n",
    "\n",
    "\\[\n",
    "F1_{\\text{Malignant}} = 2 \\cdot \\frac{\\text{Precisión} \\cdot \\text{Sensibilidad}}{\\text{Precisión} + \\text{Sensibilidad}}\n",
    "\\]\n",
    "\n",
    "- **Para *Benign***:\n",
    "\n",
    "\\[\n",
    "F1_{\\text{Benign}} = 2 \\cdot \\frac{\\text{Precisión} \\cdot \\text{Sensibilidad}}{\\text{Precisión} + \\text{Sensibilidad}}\n",
    "\\]\n",
    "\n",
    "> **Nota**: Realiza los cálculos numéricos de estos valores en tu código para mayor precisión.\n",
    "\n",
    "### 3. Interpretación de Resultados\n",
    "El modelo muestra un buen rendimiento general, con una **alta exactitud** y un **F1-Score positivo** en ambas clases. Sin embargo, se podría considerar optimizar la sensibilidad para la clase *malignant*, debido a una mayor tasa de falsos negativos (14), que podría ser relevante dependiendo del contexto de la aplicación del modelo.\n",
    "\n",
    "### 4. Conclusiones\n",
    "- **Fortalezas**: Alta exactitud y precisión, especialmente en la clase *benign*.\n",
    "- **Debilidades**: Mayor tasa de falsos negativos para la clase *malignant*, lo cual podría ser crítico dependiendo del contexto.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mejora de Interpretaciones de Métricas\n",
    "\n",
    "1. **Contextualiza las Métricas**\n",
    "   - Explica el contexto del problema: Describe el problema que estás abordando y por qué las métricas son importantes en este caso particular.\n",
    "   - Especifica el impacto de errores: Detalla cómo los falsos positivos y falsos negativos afectan a las partes interesadas. Por ejemplo, en un diagnóstico médico, un falso negativo puede tener consecuencias graves.\n",
    "\n",
    "2. **Compara con un Baseline**\n",
    "   - Establece un punto de referencia: Compara las métricas del modelo con un modelo base (por ejemplo, clasificación aleatoria o un modelo anterior). Esto ayuda a mostrar la mejora lograda.\n",
    "\n",
    "3. **Usa Visualizaciones**\n",
    "   - Incluye gráficos: Utiliza visualizaciones, como matrices de confusión, curvas ROC, y gráficos de precisión-recall, para ilustrar el rendimiento del modelo. Las visualizaciones pueden hacer que las métricas sean más comprensibles.\n",
    "\n",
    "4. **Desglosa las Métricas por Clase**\n",
    "   - Analiza por clases individuales: Presenta las métricas (precisión, recall, F1-Score) para cada clase en problemas de clasificación multiclase. Esto ayuda a identificar desbalances en el rendimiento del modelo.\n",
    "\n",
    "5. **Incluir Ejemplos Concretos**\n",
    "   - Proporciona ejemplos: Usa casos concretos para ilustrar cómo el modelo maneja ciertas predicciones. Por ejemplo, muestra un par de instancias donde el modelo tuvo éxito y algunas donde falló.\n",
    "\n",
    "6. **Relación con Objetivos de Negocio**\n",
    "   - Conecta con los objetivos: Explica cómo las métricas se alinean con los objetivos comerciales o de investigación. Por ejemplo, cómo una alta precisión podría aumentar la satisfacción del cliente.\n",
    "\n",
    "7. **Discute Limitaciones**\n",
    "   - Sé honesto sobre las limitaciones: Reconoce las limitaciones del modelo y las métricas. Por ejemplo, un modelo puede tener alta exactitud pero baja sensibilidad, lo que puede no ser adecuado para ciertas aplicaciones.\n",
    "\n",
    "8. **Resumen de Resultados**\n",
    "   - Ofrece un resumen claro: Finaliza con un resumen que destaque los hallazgos más importantes, incluyendo métricas clave y lo que significan para el contexto del problema.\n",
    "\n",
    "### Ejemplo de Interpretación Mejorada\n",
    "\n",
    "#### Interpretación de Resultados\n",
    "\n",
    "El modelo presenta una **exactitud** del 89.06%, lo que significa que, en general, el 89.06% de las predicciones fueron correctas. Este nivel de precisión es superior al modelo base, que tenía una exactitud del 75%.\n",
    "\n",
    "Sin embargo, al desglosar las métricas, encontramos que la **precisión** para la clase \"malignant\" es de aproximadamente 91.23%, mientras que para \"benign\" es de 87.72%. Esto sugiere que el modelo es más efectivo al predecir casos malignos, pero podría estar subestimando algunos casos benignos.\n",
    "\n",
    "Es crucial notar que hay un número significativo de falsos negativos (14), lo que puede ser problemático en un contexto médico donde la detección temprana de casos malignos es vital.\n",
    "\n",
    "Por lo tanto, aunque el modelo en general tiene un buen rendimiento, se recomienda realizar ajustes para mejorar la sensibilidad de la clase \"malignant\", priorizando así la reducción de falsos negativos en aplicaciones críticas.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
